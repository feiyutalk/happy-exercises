{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入工具库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理与特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_col = ['user_id', 'user_location_city',\n",
    "           'srch_destination_id', 'srch_destination_type_id', 'hotel_continent',\n",
    "           'hotel_country', 'hotel_market']\n",
    "\n",
    "num_col = ['is_mobile', 'is_package']\n",
    "\n",
    "def bin_time(t):\n",
    "    if t < 0:\n",
    "        x = 0\n",
    "    elif t < 2:\n",
    "        x = 1\n",
    "    elif t < 7:\n",
    "        x = 2\n",
    "    elif t < 30:\n",
    "        x = 3\n",
    "    else:\n",
    "        x = 4\n",
    "    \n",
    "    return x\n",
    "\n",
    "def pre_process(data):\n",
    "    try:\n",
    "        data.loc[data.srch_ci.str.endswith('00'),'srch_ci'] = '2015-12-31'\n",
    "        data['srch_ci'] = data.srch_ci.astype(np.datetime64)\n",
    "        data.loc[data.date_time.str.endswith('00'),'date_time'] = '2015-12-31'\n",
    "        data['date_time'] = data.date_time.astype(np.datetime64)\n",
    "    except:\n",
    "        pass\n",
    "    data.fillna(0, inplace=True)\n",
    "    \n",
    "    data['ci_month'] = data['srch_ci'].apply(lambda dt: dt.month)\n",
    "    data['season_dest'] = 'season_dest' + data.ci_month.map(str) + '*' + data.srch_destination_id.map(str)\n",
    "    data['season_dest'] = data['season_dest'].map(hash)\n",
    "    data['time_to_ci'] = data.srch_ci-data.date_time\n",
    "    data['time_to_ci'] = data['time_to_ci'].apply(lambda td: td/np.timedelta64(1, 'D'))\n",
    "    data['time_to_ci'] = data['time_to_ci'].map(bin_time)\n",
    "    data['time_dest'] = 'time_dest' + data.time_to_ci.map(str) + '*' + data.srch_destination_id.map(str)\n",
    "    data['time_dest'] = data['time_dest'].map(hash)\n",
    "    \n",
    "    for col in cat_col:\n",
    "        data[col] = col + data[col].map(str)\n",
    "        data[col] = data[col].map(hash)\n",
    "\n",
    "\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "cat_col_all = cat_col + ['season_dest', 'time_dest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评判标准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map5eval(preds, actual):\n",
    "    predicted = preds.argsort(axis=1)[:,-np.arange(5)]\n",
    "    metric = 0.\n",
    "    for i in range(5):\n",
    "        metric += np.sum(actual==predicted[:,i])/(i+1)\n",
    "    metric /= actual.shape[0]\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建模与预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists('../output/probs/bnb.pkl'):\n",
    "    with open('../output/probs/bnb.pkl', 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "else:\n",
    "    clf = BernoulliNB(alpha=1.0)\n",
    "#clf.sparsify()\n",
    "for epoch in range(1):\n",
    "    count = 0\n",
    "    chunksize = 200000\n",
    "    n_features = 1000000\n",
    "    #preds = np.zeros((test.shape[0], 100))\n",
    "    \n",
    "    #train = pd.read_csv('../input/train.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], skiprows=0, nrows=1000)\n",
    "    #train.columns.difference(test.columns)\n",
    "    #chunk = pd.read_csv('../input/train.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], nrows=200000)\n",
    "    print('Epoch %d started' % epoch)\n",
    "    reader = pd.read_csv('../input/train.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], chunksize=chunksize)\n",
    "    for chunk in reader:\n",
    "        try:\n",
    "            #chunk = chunk[chunk.is_booking==1]\n",
    "            #chunk = pd.merge(chunk, destinations, how='left', on='srch_destination_id')\n",
    "            #chunk = pd.merge(chunk, agg1, how='left', on='srch_destination_id')\n",
    "            pre_process(chunk)\n",
    "            #chunk = chunk[chunk.ci_year==2014]\n",
    "            y = chunk.hotel_cluster\n",
    "            sw = 1 + 4*chunk.is_booking\n",
    "            chunk.drop(['cnt', 'hotel_cluster', 'is_booking'], axis=1, inplace=True)\n",
    "            \n",
    "            XN = csr_matrix(chunk[num_col].values)\n",
    "            X = csr_matrix((chunk.shape[0], n_features))\n",
    "            rows = np.arange(chunk.shape[0])\n",
    "            for col in cat_col_all:\n",
    "                dat = np.ones(chunk.shape[0])\n",
    "                cols = chunk[col] % n_features\n",
    "                X += csr_matrix((dat, (rows, cols)), shape=(chunk.shape[0], n_features))\n",
    "            X = hstack((XN, X))\n",
    "            book_indices = sw[sw > 1].index.tolist()\n",
    "            X_test = csr_matrix(X)[book_indices]\n",
    "            y_test = y[book_indices]\n",
    "            \n",
    "            clf.partial_fit(X, y, classes=np.arange(100), sample_weight=sw)\n",
    "            #len([i for i in clf.coef_[1] if i != 0])\n",
    "            #len([i for i in clf.coef_[1] if i > 0])\n",
    "            #jb = [col for h in np.argsort(abs(clf.coef_[5])) for col in chunk.columns if (hash(col) % n_features) == h]\n",
    "            \n",
    "            #preds += np.vstack(tuple([clf.predict_proba(test.loc[i*chunksize:min((i+1)*chunksize,test.shape[0]),:]) for i in range(int(test.shape[0]/100000))]))\n",
    "            #preds += clf.predict_proba(test)\n",
    "            \n",
    "            count = count + chunksize\n",
    "            map5 = map5eval(clf.predict_proba(X_test), y_test)\n",
    "            print('%d rows completed. MAP@5: %f' % (count, map5))\n",
    "            if(count/chunksize == 200):\n",
    "                break\n",
    "        except Exception as e:\n",
    "            #e = sys.exc_info()[0]\n",
    "            print('Error: %s' % str(e))\n",
    "            pass\n",
    "\n",
    "with open('../output/probs/bnb.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "count = 0\n",
    "chunksize = 10000\n",
    "preds = np.empty((0,100))\n",
    "#chunk = pd.read_csv('../input/test.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], nrows=10000)\n",
    "reader = pd.read_csv('../input/test.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], chunksize=chunksize)\n",
    "for chunk in reader:\n",
    "    #chunk = pd.merge(chunk, destinations, how='left', on='srch_destination_id')\n",
    "    #chunk = pd.merge(chunk, agg1, how='left', on='srch_destination_id')\n",
    "    chunk.drop(['id'], axis=1, inplace=True)\n",
    "    pre_process(chunk)\n",
    "    \n",
    "    XN = csr_matrix(chunk[num_col].values)\n",
    "    X = csr_matrix((chunk.shape[0], n_features))\n",
    "    rows = np.arange(chunk.shape[0])\n",
    "    for col in cat_col_all:\n",
    "        dat = np.ones(chunk.shape[0])\n",
    "        cols = chunk[col] % n_features\n",
    "        X += csr_matrix((dat, (rows, cols)), shape=(chunk.shape[0], n_features))\n",
    "    X = hstack((XN, X))\n",
    "    \n",
    "    pred = clf.predict_proba(X)\n",
    "    preds = np.vstack((preds, pred))\n",
    "    count = count + chunksize\n",
    "    print('%d rows completed' % count)\n",
    "\n",
    "del clf\n",
    "\n",
    "if os.path.exists('../output/probs/allpreds_bnb.h5'):\n",
    "    with h5py.File('../output/probs/allpreds_bnb.h5', 'r+') as hf:\n",
    "        #print('reading in and combining probabilities')\n",
    "        predshf = hf['preds']\n",
    "        #preds += predshf.value\n",
    "        print('writing latest probabilities to file')\n",
    "        predshf[...] = preds\n",
    "else:\n",
    "    with h5py.File('../output/probs/allpreds_bnb.h5', 'w') as hf:\n",
    "        print('writing latest probabilities to file')\n",
    "        hf.create_dataset('preds', data=preds)\n",
    "\n",
    "col_ind = np.argsort(-preds, axis=1)[:,:5]\n",
    "hc = [' '.join(row.astype(str)) for row in col_ind]\n",
    "\n",
    "sub = pd.DataFrame(data=hc, index=submission.id)\n",
    "sub.reset_index(inplace=True)\n",
    "sub.columns = submission.columns\n",
    "sub.to_csv('../output/pred_sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
